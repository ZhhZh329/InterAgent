1. 我们现在正在实现一个对于一个research goal进行代码以实现sota的工作的Agent，比如你看在benchmarks里有mlebench和paperbench，但我们不会局限于一个具体的任务。我们的目标是根据一个任意的research topic，research goal，dataset来写出来可用的代码，并不断优化到最佳。我们先实现一个MVP管线，实现一个能完整跑通“任务->代码->结果”流程的最小系统。在这个MVP part中，我们先在src/mle_bench中写一个专属于mlebench的数据处理器, 数据在.cache/mle-bench下面这个处理器主要负责解压数据，别的啥都不干（由于这个处理器是专属于mlebench的因此可以这么写）。在完成数据处理之后，我们需要初始化为InterAgent类的一个CodingAgent（这个CodingAgent写在src/InterAgent下面就行，调用src/utils/openai_client.py来写代码），并且在InterAgent中留下对于一个run_research_task。InterAgent的第一步是有一个专门的函数用来分析这个任务的research topic，research goal,我们起名这个函数叫analyze_research，这个函数读取指定目录下面的一个文本文件，调用LLM分析research topic和research goal。run_research_task的第一步是写一个读数据的文件叫“load_task_data.py”这个是调用CodingAgent，根据research topic和research goal编写出来，目录放在workspaces/{dataset_name}/ 下面，注意不要产生任何其他的多余的文件了。我们先完成以上说的步骤。不确定的地方不要实现，问我做决策。

> 我们现在正在实现一个对于一个research goal进行代码以实现sota的工作的Agent，比如你看在benchmarks里有mlebench和paperbench，但我们不会局限于一个具体的任务。我们的目标是根据一个任意的research topic，research goal，dataset来写出来可用的代码，并不断优化到最佳。我们先实现一个MVP管线，实现一个能完整跑通“任务->代码->结果”流程的最小系统。 我们先到第一步，我们确认InterAgent的输入，InterAgent的输入我们只要知道research topic，research goal还有dataset的位置对吧。那么我们在mlebench那边写一个mle file_agent.py文件来告诉InterAgent这三个关键的初始化项目。所以mle file agent需要根据给定的dataset name找到路径，分析出来researchtopic, research goal还有拼出来dataset的路径，并且把这些东西交给InterAgent。那么在main.py里我们需要的第一步就是告诉main函数，我正在测试mlebench的东西，因此需要调用mle file agent，而不是其他的如paper bench agent,我们现在就实现这么点东西，ok?分析的部分用src/utils/openai client.py调用llm分析
   
2. 作为对一次传入的research topic，research goal，dataset先打印出来前几条和分析这个task要完成的目标。另外，要注意dataloader同时要传入很多的其他知识，比如通过调用llm（via src/utils/openai_client.py）分析description.md，得到research topic，research goal，dataset。告诉InterAgent，sample_submission.csv是mlebench要填写的用于评估的文件，需要填写这个。

3. 我觉得直接继承现在的mle_file_agent增加传递根据description.md分析出来要干啥（research topic），目标是啥（research goal），以及数据集怎么读取就行。然后使用logger管理打印的内容，并且输出全部用中文，而不要用英文
4. 好的接下来吧我们继续实现这个最小MVP，我把src/InterAgent/coding_agent.py这个之前旧版本的codingagent整了过来，你可以看看他是如何实现的基本功能的，但不要关注所有有关路径和memory相关的内容。然后把实现基本功能（debug，写代码）提取出来（以函数的形式保留，也就是一个class里有多个接口可以调用）然后在这个基础上，整合到InterAgent里，是的我们的代码可以完成结果的部分，并且这次必须强制agent在对应的workspace里生成代码时（不是说codingagent，说的是它生成的代码）只有一个文件（就是除了load data以外，只有一个code file，确保后续debug的时候不会出现多个文件同时出现问题导致不好改），这个被codingagent生成出来的code file能调用data loader的数据，然后在root path下的uv python执行之后可以在对应的workspace目录下输出sample_submission.csv（也就是说在例如这样一个路径下面workspaces/aerial-cactus-identification/exp_20250929_175615_eed77f13， 除了对应的code file.py就只有根据dataset那边的sample_submission.csv，前面的两个分析了）.另外我要你把workspace_info.json放到cache里，不许在root目录里给我乱生成东西。
5. 
6. Great，现在我尝试了好几个不太典型的任务好像都可以正常读取进来数据了。让我们回归实现MVP的主线任务，接下来我们在初始数据读取完之后有一些简要的数据集加载测试结果，这个部分现在是通过Debug=True打印出来的对吗，你只要回答我是或否？
7. 我觉得对于一个做research task的“人”来说，在简单地浏览数据结构和几个case，对数据有感知之后，可以把这些数据结构和research topic，research task都进行一个初步的分析，因此我想把Debug=True打印出来的内容，在debug=False也保留打印（也就是说打印数据集的初始结构将一直保留，无论debug是什么），并作为一个参数传递出来。此时对于我们说的初步的分析，我们可以在InterAgent中设定一个函数init_analyzing，这个函数只有在load_data写work了之后，调用唯一的一次，我们通过prompt，让InterAgent对这个部分进行初步分析，并且通过日志打印出来这个分析。
8. 接下来，你把初步的分析结果，research topic，research goal，数据集结构的信息，都交给codingagent的generated code，来让codingagent根据要求去完成research goal的目标。如果现在的coding agent没有支持我最新的描述，那么请你根据最新的要求修改codingagent的function，codingagent完成generate之后应该在workspaces/{benchmark_name}/{dataset_name}/exp_id_xxx/ 这个路径下面生成一个唯一文件research.py（所有的代码all in one file，来确保方便修改），这个research.py运行之后，应该根据对应的数据集的sample_submission.csv（有的不一定叫这个，比如我们现在测试的text-normalization-challenge-english-language）文件生成一个对应的带结果的sample_submission.csv（follow在读数据那边的名字就行）。也就是说，在step1，对应的workspaces/{benchmark_name}/{dataset_name}/exp_id_xxx/路径下面只有如何load_research_data.py和research.py，然后InterAgent调用research.py（对于InterAgent只是调用research.py），生成对应的sample_submission.csv（这个是mlebench的特性，生成就叫sample_submission.csv这个名字的这个参数实际上不应该写死到coddingagent里面，而是InterAgent通过分析）如果有不确定的你先问我，先别写。
9. 接下来我们在src/mle_bench下面写一个mle_evaluator.py用于对InterAgent的结果进行评估，这个mle_evaluator不要和InterAgent耦合的太死，我们只是给InterAgent留一个Evaluator的类内属性，这个evaluator可能用于
10. 好的，我现在尝试了多个task至少都能跑到research.py，大部分都能进行评估，并且获取结果。并且有run_research_task这个主入口。现在有几个问题需要你再帮我确认一下，如果有问题我们再修改。
    1.  第一，high-level耦合问题：我现在的思路是InterAgent是一个完全只协调做research task的Agent，至于是什么任务，评估结果如何，都是靠src下面对应写好的 {benchmark_name}/对应的文件（例如src/mle_bench） 返回给我。如果存在耦合问题，请你再次解耦合。
    2.  第二，规范问题。我想进一步让run_research_task的可读性更加完美，具体来说，我想